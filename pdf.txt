Alan’s Speakeasy - An Ecosystem for the
Evaluation of Conversational Agents

Idea
With the increasing interest in conversational agents, it became relevant to have a
framework to evaluate and compare conversational agents (aka bots). We implemented
an ecosystem including (1) a Web-based platform that hosts chatrooms where humans and
conversational agents can interact for humans to evaluate the agents in a Turing-test style,
and (2) a set of artificial agents whose goal is to help in the development and evaluation of
conversational bots


Speakeasy Capabilities
• Generation of conversational sessions by pairing users (i.e., humans and bots) and
configuration of conversations (e.g., conversation duration, evaluation dimensions).
• Human feedback provision at the level of a conversational agent message (i.e.,
like/dislike/star) and entire conversation (i.e., through a customizable survey containing
Likert-scale questions).
• Automated feedback provision through Assessor Bots. Assessor Bots can 1) assist human
evaluators in their assessment of other agents, 2) conduct evaluations autonomously,
and 3) assist students with their conversational agents‘ development.
• Customizable data generation pipeline for preparing suitable datasets based on
Wikidata and movie-related sources.

Architecture
• Frontend implemented in TypeScript
using Angular framework for browser-
based user interface
• Backend implemented in Kotlin
running on Java Virtual Machine with
Javalin as HTTP server
• Offers RESTful API with minimal
requirements for agent integration
• Stores message data (content, author,
timestamp, chatroom) which can be
exported as JSON for analysis
• Anonymizes conversations by assigning
unique session names and supports
multimodal content (e.g., text, images,
audio, and video)

(a) GUI for the conversation between a human user and a bot. (b) Survey-based conversation assessment. The assessment dimensions can be configured. The dimensions we
defined are accuracy, completeness, timeliness, human-likeness. Additionally, we included the dimensions regarding the bot capabilities relevant in the teaching environment (e.g.,
knowledge graph-based question answering, embeddings, recommendations, multimodal processing, and crowdsourcing). (c) The evaluator bot asks the bot to be assessed
questions, explains the extent to which the are answers are correct or incorrect, and shares the correct answers when needed. (d) The assistant bot helps the human assessing the
bot with suggestions on what to ask the bot. (e) The administrator of an evaluation event can monitor assessment statistics and conversations.

Results and Takeaways
• Successfully deployed in educational settings for a Master’s course across 4 years with 337 participants evaluating 114 conversational agents
• Speakeasy differentiates itself from other platforms by its easy-to-use API which let’s anyone test their own conversational agent as well as
allowing humans to interact with and assess conversational agents in real time.
• Provides a flexible, open-source ecosystem for conversational multimodal agents that can be exte